---
title: "classification_experiement_v9 - implement DIMVf"
output:
  pdf_document: default
  html_document: default
date: "2022-09-30"
---


```{r setup, include = FALSE}
knitr::opts_chunk$set(cache = TRUE, echo=TRUE, eval = TRUE)
``` 


```{r}
require(knitr)

purl("imputation_knn.Rmd", output = 'imputation_knn.R')
```

```{r}

packages <- c(
  "missMDA", 
  "softImpute", 
  "caret", 
  "caTools", 
  "glue", 
  "jsonlite", 
  "future.apply", 
  "dslabs", 
  "cowplot", 
  "magick", 
  "progress"
)
# Install packages not yet installed
installed_packages <- packages %in% rownames(installed.packages())
if (any(installed_packages == FALSE)) {
  install.packages(packages[!installed_packages])
}

# Packages loading
invisible(lapply(packages, library, character.only = TRUE)) 

library(here) 
source(here('src/rscript/dimv2.R'))  
source(here('src/rscript/dpers.R'))   
source(here('src/rscript/utils.R'))    
source(here('src/rscript/imputation_comparation2.R'))     

plan(multisession, workers = 4)  

```

IF HASNOT DOWNLOAD THE FILE YET THEN 
```{r}
#getting the path to save 
# SETTING 
curr_dir = getwd()
path = '../../../data/mnist/raw/'
ROOT = '../../../data/mnist/imputed/' 
FILE_NAME = 'v13'
```


```{r}
# mnist_path = file.path(curr_dir, path) 
# print(mnist_path)
# 
# if (!file.exists(file.path(mnist_path, "train-images-idx3-ubyte")) |
#   !file.exists(file.path(mnist_path, "t10k-images-idx3-ubyte")) |
#   !file.exists(file.path(mnist_path, "train-labels-idx1-ubyte")) |
#   !file.exists(file.path(mnist_path, "t10k-labels-idx1-ubyte")) 
#   ){
#   
#   # getting the data 
#   mnist <- read_mnist(
#     path = NULL,
#     destdir = mnist_path, 
#     download = TRUE,
#     url = "https://www2.harvardx.harvard.edu/courses/IDS_08_v2_03/",
#     keep.files = TRUE
#   )  
#   
#   # clear folder data (avoid wrong zipping)
#   list_files = list.files(path=mnist_path) 
#   for (x in 1:length(list_files)){
#     file_path = file.path(mnist_path, list_files[x]) 
#     if (substring(file_path, nchar(file_path)-2, nchar(file_path)) == '.gz'){
#       R.utils::gunzip(file_path, overwrite=TRUE, remove=FALSE) 
#     }
#     }  
# } 
  
```


IF FILE IS ALREADY DOWNLOADED AND UNZIP THEN JUST READ 

```{r}
# load image files
# load_image_file = function(filename) {
#   ret = list()
#   f = file(filename, 'rb')
#   readBin(f, 'integer', n = 1, size = 4, endian = 'big')
#   n    = readBin(f, 'integer', n = 1, size = 4, endian = 'big')
#   nrow = readBin(f, 'integer', n = 1, size = 4, endian = 'big')
#   ncol = readBin(f, 'integer', n = 1, size = 4, endian = 'big')
#   x = readBin(f, 'integer', n = n * nrow * ncol, size = 1, signed = FALSE)
#   close(f)
#   data.frame(matrix(x, ncol = nrow * ncol, byrow = TRUE))
# }
# 
# # load label files
# load_label_file = function(filename) {
#   f = file(filename, 'rb')
#   readBin(f, 'integer', n = 1, size = 4, endian = 'big')
#   n = readBin(f, 'integer', n = 1, size = 4, endian = 'big')
#   y = readBin(f, 'integer', n = n, size = 1, signed = FALSE)
#   close(f)
#   y
# } 

```

```{r}
# load images
# processing_mnist_data <- function (){
#   train = load_image_file(file.path(mnist_path, "train-images-idx3-ubyte"))
#   test  = load_image_file(file.path(mnist_path,"t10k-images-idx3-ubyte")) 
# 
#   train$label =  as.factor(load_label_file(file.path(mnist_path,"train-labels-idx1-ubyte")))
# 
#   test$label = as.factor(load_label_file(file.path(mnist_path,"t10k-labels-idx1-ubyte")))
#   result = list('train'=train, 'test'=test)
#   return(result)
# }
``` 

```{r}
# processed_data = processing_mnist_data()
# train = processed_data$train 
# test = processed_data$test
# X.train = train[, -785]
# X.test = test[, -785]
# y.train = train[, 785, drop=F]
# y.test = test[, 785, drop=F] 
``` 




# DATA PREPARATION
## READING DATA : 

## CREATE NON RANDOM MISSING VALUE 

```{r}
# get_image_position_spatial_to_flatten<- function(delImgPosWidth, delImgPosHeight){ 
#   # delImgPosHeight: row 
#   # delImgPosWeight : col 
#   tmp = c(1:784)
#   im <- matrix(unlist(tmp),nrow = 28,byrow = T)
#   idxs = im[delImgPosHeight, delImgPosWidth]  
#   return(matrix(idxs,nrow = 1,byrow = T)[, ])
# }
```  

```{r}
# image_edge_deleting <- function(
#     data, 
#     delete_type, #by_percent, by_pixel_number 
#     percents_of_data,
#     image_width,
#     image_height,
#     width_del_percent=0, 
#     height_del_percent=0,
#     from_pixel_width=None, 
#     from_pixel_height=None
#     ){
#   if (delete_type =='by_percent'){
#     n = dim(data)[2]
#     from_pixel_width = ceiling((1-width_del_percent)*image_width)   
#     from_pixel_height = ceiling((1-height_del_percent)*image_height)
#   }
#   if (delete_type=='by_pixel_number'){
#     from_pixel_width = from_pixel_width
#     from_pixel_height = from_pixel_height
#   }
#   
# flatten_columns_removed = get_image_position_spatial_to_flatten(
#   from_pixel_width:image_width,
#   from_pixel_height: image_height
# )
# 
#   flatten_rows_removed = sample.int(nrow(data), as.integer(nrow(data)*percents_of_data))
#   missing_data = data
#   missing_data[flatten_rows_removed, flatten_columns_removed] <- NA
#   result = list(
#     'missing_data'=missing_data, 
#     'flatten_columns_removed'=flatten_columns_removed,  
#     'flatten_rows_removed'=flatten_rows_removed
#   ) 
#   return(result)
# }

``` 

`

```{r}
# visualize the deleted images 
# visualize_digit <- function(missing_X, y, train_removed_rows, per_col, per_row, title){
# 
#   par(mfcol=c(per_col, per_row))
#   par(mar=c(0, 0, 3, 0), xaxs='i', yaxs='i')  
#  
#   for (idx in 1:(per_col*per_row)) { 
#       im <- matrix(unlist(missing_X[train_removed_rows, ][idx, ]),nrow = 28,byrow = T)
#       im <- t(apply(im, 2, rev)) 
#       image(1:28, 1:28, im,  xaxt='n', main=paste(y[train_removed_rows,,drop=F][idx, ])) 
#       #image(1:28, 1:28, im, col=gray((0:255)/255), xaxt='n', main=paste(y[train_removed_rows,,drop=F][idx, ]))
#   }
# 
# } 

``` 




```{r}

# sampling_data <- function(data, y_col_name, sample_perc){
#   data$label= data[, y_col_name]
#   sample_indices <- sample.split(Y=data[, 'label'], SplitRatio = sample_perc)
#   sample_data <- as.data.frame(subset(data, sample_indices == TRUE))
#   result = list(
#     'sample'=sample_data, 
#     'X'=as.matrix(sample_data[, 1:(28*28)]), 
#     'y'=sample_data[,'label', drop=F]
#   )
#   return(result)
# }

```



Normalizing train and test using train 's parameters 
```{r}
# normalizing <- function(x=None, Xtrain=None){
#   na_mask = is.na(x)
#   mean = apply(Xtrain, 2, mean, na.rm=TRUE)
#   sd = apply(Xtrain, 2, sd, na.rm=TRUE)
# 
#   sd_equal_zero_mask = which(sd==0)
#   subtract_mean = sweep(x, 2, mean, '-')
#   X_normed = sweep(subtract_mean, 2, sd, "/")
# 
#   X_normed[is.na(X_normed)] = 0
#   X_normed[is.infinite(X_normed)] = 0
#   X_normed[na_mask] = NA
#   result = list('X_normed'=X_normed, 'mean'=mean, 'sd'=sd, 'sd_equal_zero_mask'=sd_equal_zero_mask)
#   return (result)
# }
# 
# reconstructingNormedMatrix <- function(X_norm, mean, std){
#   mult = sweep(X_norm, 2, std, '*')
#   reconstrc = sweep(mult, 2, mean, '+')
#   return (reconstrc)
# }
```




# FULL PIPELINE ON FULL DATASET MNIST: 

```{r}
# mnistDataPreparation <- function(
#     width_del_percent=None,
#     height_del_percent=None,
#     sample_deleted_percent=None,
#     X.train,
#     X.test,
#     y.train,
#     y.test
#     ){
#   X_train = as.matrix(X.train)
#   X_test = as.matrix(X.test)
#   y_train = as.matrix(y.train)
#   y_test = as.matrix(y.test)
# 
#   #cut a piece of image
#   removed_train = image_edge_deleting(
#     X_train,
#     'by_percent',
#     sample_deleted_percent, 28, 28,
#     width_del_percent=width_del_percent,
#     height_del_percent=height_del_percent)
# 
#   removed_test= image_edge_deleting(
#     X_test,
#     'by_percent',
#     sample_deleted_percent, 28,28,
#     width_del_percent=width_del_percent,
#     height_del_percent=height_del_percent)
# 
#   train_removed_rows = removed_train$flatten_rows_removed
#   test_removed_rows = removed_test$flatten_rows_removed
#   train_removed_columns = removed_train$flatten_columns_removed
#   test_removed_columns = removed_test$flatten_columns_removed
# 
#   missing.X_train = removed_train$missing_data
#   missing.X_test =  removed_test$missing_data
#   # normalization
#   train_normed = normalizing(x=missing.X_train,Xtrain=missing.X_train)
#   missing.X_train_normed = train_normed$X_normed
#   missing.X_train_mean = train_normed$mean
#   missing.X_train_sd = train_normed$sd
# 
#   test_normed = normalizing(x=missing.X_test, Xtrain=missing.X_train)
#   missing.X_test_normed = test_normed$X_normed
# 
#   result = list(
#     "missing.X_train_normed" = missing.X_train_normed,
#     "y_train" = y_train,
#     "missing.X_test_normed" = missing.X_test_normed,
#     "y_test" = y_test,
#     "train_removed_rows" = train_removed_rows,
#     "test_removed_rows" = test_removed_rows,
#     "missing.X_train_mean" = missing.X_train_mean,
#     "missing.X_train_sd" = missing.X_train_sd
#   )
#   return(result)
# }

```

```{r}
# 
# sub_path = "../../../data/mnist/imputed/v13/threshold_10_deletedWidthHeightPc_6060_noImagePc_50"
# path.y_train = file.path(sub_path, 'y_train.csv.gz')
# r = read.table(path.y_train , sep=" ", header=T)
# as.vector(r)$X0
# path.X_test_normed  = file.path(sub_path, 'y_test.csv.gz')     
```


```{r}
imputationPipeline<- function(
    width_del_percent=None,
    height_del_percent=None,
    sample_deleted_percent=None,
    correlation_threshold=None
    # X.train,
    # X.test,
    # y.train,
    # y.test
){
  width_del_percent = width_del_percent
  height_del_percent = height_del_percent
  sample_deleted_percent = sample_deleted_percent
  # X.train = X.train 
  # X.test = X.test
  # y_train = y.train 
  # y_test = y.test 
  
  # missingDataCreated = mnistDataPreparation( 
  #     width_del_percent=width_del_percent, 
  #     height_del_percent=height_del_percent, 
  #     sample_deleted_percent=sample_deleted_percent, 
  #     X.train, 
  #     X.test, 
  #     y.train, 
  #     y.test
  # )
  print(paste0(
    "Starting imputation with weight and weight pc : ", 
    width_del_percent, 
    ",  sample deleted percent: ", 
    sample_deleted_percent, 
    ", correlation threshold: ", 
    correlation_threshold
    )) 
  #----
  width_del = toString(as.integer(width_del_percent*100))
  heigh_del =  toString(as.integer(height_del_percent*100)) 
  percent_img_del =  toString(as.integer(sample_deleted_percent*100)) 
  threshold =  as.integer(correlation_threshold*100) 
  
  threshold_string = if (as.integer(threshold/10) < 1){paste0("0", toString(threshold))}else{toString(threshold)} 
  
  sub_folder = paste0(
    "threshold_", 
    threshold_string, 
    "_deletedWidthHeightPc_", 
    width_del, 
    heigh_del, 
    '_noImagePc_', 
    percent_img_del
  )
  curr_dir = getwd() 
  main_dir = paste0(ROOT, FILE_NAME)
  sub_path = file.path(curr_dir, main_dir, sub_folder)
  print(sub_path)
  if (dir.exists(sub_path)==F){
    dir.create(sub_path)
  }
  
  #------
  # missing.X_train_normed=missingDataCreated$missing.X_train_normed 
  # y_train = missingDataCreated$y_train
  # 
  # missing.X_test_normed = missingDataCreated$missing.X_test_normed
  # y_test = missingDataCreated$y_test
  # 
  # train_removed_rows = missingDataCreated$train_removed_rows
  # test_removed_rows = missingDataCreated$test_removed_rows
  # 
  # missing.X_train_mean = missingDataCreated$missing.X_train_mean
  # missing.X_train_sd = missingDataCreated$missing.X_train_sd
  #---------------------------------------------------------------
  path.X_train_normed = file.path(sub_path, 'X_train_normed.csv.gz')
  path.X_test_normed  = file.path(sub_path, 'X_test_normed.csv.gz') 
  path.y_train  = file.path(sub_path, 'y_train.csv.gz')
  path.y_test  = file.path(sub_path, 'y_test.csv.gz')          
  # path.X_train_mean = file.path(sub_path, 'X_train_mean.csv.gz')
  # path.X_train_sd = file.path(sub_path, 'X_train_sd.csv.gz')
  # 
  
  missing.X_train_normed = as.matrix(read.table(path.X_train_normed, sep=' ', header=TRUE))
  missing.X_test_normed  = as.matrix(read.table(path.X_test_normed, sep=' ', header=TRUE)) 
  y_train = as.matrix(read.table(path.y_train, sep=' ', header=TRUE)) 
  y_test  = as.matrix(read.table(path.y_test , sep=' ', header=TRUE)) 
  # missing.X_train_mean   = as.matrix(read.table(path.X_train_mean, sep=' ', header=TRUE)) 
  # missing.X_train_sd     = as.matrix(read.table(path.X_train_sd, sep=' ', header=TRUE)) 
  # 
 # test_removed_rows = as.vector(read.csv(file.path(sub_path, 'test_removed_rows.csv')))$test_removed_rows
  print("check dim")
  print(dim(missing.X_train_normed))
  print(dim(missing.X_test_normed))
  print(dim(y_train))
  print(dim(y_test))
  
  #---------------------------------------------------------------------------------------------------------------  
  print("start knn ")
  START = Sys.time()   
  
  result_knn =  kNNimpute_run(
    missing.X_train_normed, 
    as.vector(y_train)$X0,  
    missing.X_test_normed, 
    as.vector(y_test)$X0
  ) 

  duration_KNN = Sys.time() - START
  print(duration_KNN)
  write.table(result_knn$train, file.path(sub_path, 'train_knn.csv.gz'), row.names=FALSE)
  write.table(result_knn$test, file.path(sub_path, 'test_knn.csv.gz'), row.names=FALSE)
  #---------------------------------------------------------------------------------------------------------------  
  # print("start dimv")
  # START = Sys.time()
  # result_knn = impDi_run(
  #   as.matrix(missing.X_train_normed),
  #   as.vector(y_train)$label,
  #   as.matrix(missing.X_test_normed),
  #   as.vector(y_test)$label
  # )
  # impDiTime = Sys.time() - START
  # print(impDiTime)


  # imputed data

}
```


```{r}


 width_height_percentages =c(.6, .5, .4)
 sample_deleted_percentages = c(.5)
 correlation_threshold =c(.1)

 
for (width_height_pc in width_height_percentages){
  for (sample_pc in sample_deleted_percentages){
    for (th in correlation_threshold){
      imputationPipeline(
        width_del_percent=width_height_pc,
        height_del_percent=width_height_pc,
        sample_deleted_percent=sample_pc,
        correlation_threshold=th
        # X.train,
        # X.test,
        # y.train,
        # y.test
        )
    }

  }

}


```

